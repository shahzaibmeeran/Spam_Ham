{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6e00a-da50-4a06-ab01-c1c221ee7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e977ad0-6265-4614-be91-d87c73040aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca38ec-b690-48fe-a6f9-5684c21b01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d592c-da5a-44a8-a7ce-5cc67c150d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam_ham_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95aa0b3-b48e-4dde-aabd-82cc85a3e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4015d628-d7b2-4010-9b85-ab21a1d8a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327fd8b-63ae-4a22-8926-ad2a0c93a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f47cb6-37ae-4f40-8b57-0dd7b8e3537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fb48c-034f-41c4-9e48-62737963b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bc1d8-59ca-4887-a9e8-0db725dd8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1bf66-7aa1-4c87-9065-a21aaef6edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afbe0f-7e13-4aab-a3c7-b4013b9ed01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='label', discrete=True, stat='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01af6df-7cac-4d60-9ef6-3e9197763d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='label_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed9108-6e0b-4e99-ab45-ef978cb93fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd6b3b-1e48-4245-936d-96a820019c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "# print(stopwords.words('english'))\n",
    "\n",
    "STOPWORDS = stopwords.words('english')\n",
    "STEMMER = PorterStemmer()\n",
    "LEMMANTIZER = WordNetLemmatizer()\n",
    "\n",
    "def process_text(\n",
    "    text, tokenized=False, lowercase=True, remove_punctuation=True,\n",
    "    remove_digit=True, remove_stopwords=True, add_stopwords=[],\n",
    "    ngrams=None, use_stem_lemmantize='stem',\n",
    "):\n",
    "    stopwords_ = set(STOPWORDS).union(set(add_stopwords))\n",
    "    stopwords_ = set([s.lower() for s in stopwords_])\n",
    "    # normalize, lowercase, tokenize, remove punctuation\n",
    "    tokens = unicodedata.normalize('NFKC', text)\n",
    "    tokens = tokens.lower() if lowercase else tokens\n",
    "    if not tokenized:\n",
    "        if remove_punctuation:\n",
    "            pattern = '[{}\\s]+'.format(punctuation)\n",
    "        else:\n",
    "            pattern = '([{}]+)'.format(punctuation)\n",
    "            tokens = re.sub(pattern, r' \\1 ', tokens)\n",
    "            pattern = '\\s+'\n",
    "        tokens = re.split(pattern, tokens)\n",
    "    # processs\n",
    "    p_tokens = list()\n",
    "    for token in tokens:\n",
    "        # remove digit\n",
    "        if remove_digit:\n",
    "            token = re.sub('\\d+', '', token)\n",
    "        # remove stopwords\n",
    "        if remove_stopwords:\n",
    "            token = '' if token.lower() in stopwords_ else token\n",
    "        p_tokens.append(token)\n",
    "    tokens = [t for t in p_tokens if len(t) > 0]\n",
    "    # stemming or lemmantizing\n",
    "    if use_stem_lemmantize == 'stem':\n",
    "        tokens = [STEMMER.stem(t) for t in tokens]\n",
    "    elif use_stem_lemmantize == 'lemmantize':\n",
    "        tokens = [LEMMANTIZER.lemmatize(t) for t in tokens]\n",
    "    # make n-gram\n",
    "    if not (ngrams is None):\n",
    "        ngrams_list = []\n",
    "        min_ngram, max_ngram = ngrams\n",
    "        for n in range(min_ngram, max_ngram + 1):\n",
    "            ngrams_list.append(\n",
    "                [' '.join(tokens[i: i+n])\n",
    "                 for i in range(len(tokens) - n)]\n",
    "            )\n",
    "        tokens = [t for ngrams_l in ngrams_list for t in ngrams_l]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a94e5-66f4-4156-9cbc-d7ac60f887a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "i = 605\n",
    "text = df.loc[i, 'text']\n",
    "\n",
    "print(process_text(text, remove_punctuation=True,\n",
    "                   ngrams=(1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c39a1-fdb3-4f6b-8f24-0c4dccb6e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.label\n",
    "y = df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb69c2-b581-4890-ae99-21d7656f2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['label_num'], test_size=0.3,\n",
    "    random_state=2023\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78716335-5b5f-4634-8cd1-da54a772dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer=lambda x: process_text(x, ngrams=(1, 2),\n",
    "                                    use_stem_lemmantize='stem'),\n",
    "    max_df=0.85,\n",
    "    min_df=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20a5d5-abff-4d61-bd2c-65fbec1c1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de1ea4-6c62-4b58-af5d-d9471bbcca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f560ebc-8eb6-4caa-92ac-f2a1781967e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and train model\n",
    "nb_model = MultinomialNB(alpha=0.01)\n",
    "nb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2634aa5-1521-4579-9bdc-6483e07d6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = sorted(df['label'].unique())\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83d78b-cac1-49d2-9635-e4ae65f013fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e143ef-9f01-488e-b2e9-9a9e80b88768",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b476e-a139-4d09-84a4-720a14f27edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, columns=label_names, index=label_names)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "sns.heatmap(df_cm, cmap=\"Blues\", annot=True, fmt='.0f'); # font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901268d4-8c61-4909-999c-3dd346a09f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
